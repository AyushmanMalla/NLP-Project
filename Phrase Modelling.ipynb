{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import textract\n",
    "import re\n",
    "import nltk\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath=r\"C:\\Users\\Ayushman\\Documents\\ResearchAssistant\\DrSupraja\\EdeX\\DatasetsQuali\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonstem=pd.DataFrame(columns=['reflection','stem'])\n",
    "reflections=[]\n",
    "for filename in os.listdir(os.path.join(folderpath, 'NON-STEM')):\n",
    "        file_path = os.path.join(folderpath,'NON-STEM', filename).replace('\\\\', '/')\n",
    "        text = textract.process(file_path).decode('utf-8')\n",
    "        reflections.append(text)\n",
    "df_nonstem['reflection']=reflections\n",
    "df_nonstem['stem']=0\n",
    "\n",
    "reflections=[]\n",
    "df_stem=pd.DataFrame(columns=['reflection','stem'])\n",
    "for filename in os.listdir(os.path.join(folderpath, 'STEM')):\n",
    "        file_path = os.path.join(folderpath, 'STEM', filename).replace('\\\\', '/')\n",
    "        text = textract.process(file_path).decode('utf-8')\n",
    "        reflections.append(text)\n",
    "df_stem['reflection']=reflections\n",
    "df_stem['stem']=1\n",
    "\n",
    "df_combined=pd.concat([df_stem,df_nonstem], ignore_index=True)\n",
    "df_teamwork_comb=df_combined.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    text=re.sub('(<.*?>)', ' ',text)\n",
    "    text=re.sub('[,\\.!?:()\"]', ' ',text)\n",
    "    text=text.strip()\n",
    "    text=re.sub('[^a-zA-Z\"]', ' ',text)\n",
    "    text=text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['reflection'] = df_combined['reflection'].apply(remove_special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cc     individual reflection  through cc      science and tech for humanity  i encountered many challenges  but also experienced tremendous personal growth  i was initially excited to delve into the complexities of science and technology  and their interconnectedness with humanity  through the collaborative work with students from different disciplines  after meeting with my team members from varying majors such as business  engineering  and economics  i realised that we have many differences to bridge  in terms of our strengths  working styles and expectations from this module  besides the daunting group projects and weekly assessments  the content was also something i needed time to get used to    having completed some icc modules before  i initially thought cc     would be similar  but the need to incorporate and apply interdisciplinary thinking made teamwork much harder in the beginning  this issue was something my team failed to address when working on our first proposal  resulting in conflicts and disagreements through much of our time together since we needed much time to work through our differences in thinking  one such instance was when our team was unable to come to a consensus on our problem statement and solutions in the ai proposal  and we had to work through suggestions on mental health and comprehensive care before settling on dispensing errors    however  these challenges as a group only made us stronger as we learnt to confront our problems by the submission of the ai proposal  eventually working effectively together as we learnt to leverage on each other s strengths on solution ideation  public speaking and graphic design  this ultimately allowed us to put up a poster presentation which our group was satisfied with by effective division of work  through the growth of our group  i learnt that working with different people can often cause conflicts  and the way to resolve them is through open communication and mutual understanding   beyond the teamwork aspect of my learning journey  i felt that i had gained a lot more interdisciplinary insights than what i had initially expected to learn from this module  as a science student  i was particularly interested in the first topic of ageing  as the content was well connected to much of my personal experiences in the stem aspect of the topic  however  the exploration of how other disciplines like business and social sciences can affect such a topic allowed me to step beyond my comfort zone through discussions of real world impacts  i would not have thought that the problem of ageing goes so far beyond the simple medical and social context  even providing insights on effectiveness of communication with the elderly population  the application of these lecture content in weekly team missions and ira tra assessments really allowed me to apply and exercise interdisciplinary thinking in fun and engaging ways  i remembered our team even came up a short skit to demonstrate the importance of communicating policies to elderly in the ageing topic even though none of us came majored in communications   ultimately  through my time in cc      i felt that i had grown not just as a science student  but also as a global citizen living in a state where interdisciplinary approaches to real world problems is very essential to the workings of our society  the personal growth in terms of interpersonal relationships management was invaluable  and the module has really provided me with a safe space to grow and develop  as we move on from the end of this module  i will learn to apply and make use of these knowledge gained  not forgetting the challenges and struggles which has shaped me to be who i am today'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df_combined['reflection'][3]\n",
    "x = remove_special_chars(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayushman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ayushman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayushman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "def tagged_lemma(string):\n",
    "    \n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(string))\n",
    "\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "    lemmatized_sentence = []\n",
    "\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:       \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df_combined['reflection'].apply(lambda x: text_to_word_sequence(x))\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['cc','course','module','also'])\n",
    "stop_words = set(stop_words)\n",
    "filtered_words = words.apply(lambda x: [w for w in x if not w in stop_words])\n",
    "df_combined['reflection'] = filtered_words.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonstem=df_combined[df_combined['stem']==0]\n",
    "df_stem=df_combined[df_combined['stem']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "massive_essay = ''\n",
    "for essay in df_combined['reflection']:\n",
    "    massive_essay = massive_essay + ' ' + essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "essays = df_nonstem['reflection'].tolist()\n",
    "\n",
    "# Create Phrases model with hyperparameters\n",
    "phrases = Phrases(massive_essay, min_count=1, threshold=3)  # Adjust parameters as needed\n",
    "\n",
    "# Apply Phrases model to each essay (tokenization included)\n",
    "# processed_essays = [[phrase for phrase in phrases[essay.split()]] for essay in essays]\n",
    "\n",
    "# processed_essays = [for essay in essays:\n",
    "#                         for phrase in phrases[essay.split()]:\n",
    "#                             phrase]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table',\n",
       " 'contents',\n",
       " 'introduction',\n",
       " 'content',\n",
       " 'teamwork',\n",
       " 'proposal',\n",
       " 'happened',\n",
       " 'improvements',\n",
       " 'extensions',\n",
       " 'conclusion',\n",
       " 'proposal',\n",
       " 'poster',\n",
       " 'happened',\n",
       " 'improvements',\n",
       " 'extensions',\n",
       " 'conclusion',\n",
       " 'introduction',\n",
       " 'rather',\n",
       " 'different',\n",
       " 'icc',\n",
       " 'courses',\n",
       " 'content',\n",
       " 'way',\n",
       " 'lesson',\n",
       " 'conducted',\n",
       " 'although',\n",
       " 'previous',\n",
       " 'modules',\n",
       " 'involves',\n",
       " 'groupwork',\n",
       " 'ml',\n",
       " 'rather',\n",
       " 'different',\n",
       " 'extensive',\n",
       " 'reflection',\n",
       " 'split',\n",
       " 'two',\n",
       " 'sections',\n",
       " 'content',\n",
       " 'teamwork',\n",
       " 'respectively',\n",
       " 'ensure',\n",
       " 'full',\n",
       " 'coverage',\n",
       " 'content',\n",
       " 'late',\n",
       " 'comer',\n",
       " 'group',\n",
       " 'presented',\n",
       " 'opportunity',\n",
       " 'assume',\n",
       " 'role',\n",
       " 'however',\n",
       " 'gravitated',\n",
       " 'towards',\n",
       " 'strategy',\n",
       " 'planner',\n",
       " 'tech',\n",
       " 'advisor',\n",
       " 'primarily',\n",
       " 'proposals',\n",
       " 'poster',\n",
       " 'substantially',\n",
       " 'influenced',\n",
       " 'ideas',\n",
       " 'introduced',\n",
       " 'understanding',\n",
       " 'ai',\n",
       " 'technology',\n",
       " 'limited',\n",
       " 'basic',\n",
       " 'functionalities',\n",
       " 'however',\n",
       " 'illuminated',\n",
       " 'pervasive',\n",
       " 'influence',\n",
       " 'technology',\n",
       " 'across',\n",
       " 'various',\n",
       " 'sectors',\n",
       " 'delving',\n",
       " 'multifaceted',\n",
       " 'challenges',\n",
       " 'ageing',\n",
       " 'particularly',\n",
       " 'enlightening',\n",
       " 'revealed',\n",
       " 'myriad',\n",
       " 'approaches',\n",
       " 'undertaken',\n",
       " 'diverse',\n",
       " 'stakeholders',\n",
       " 'scientists',\n",
       " 'governmental',\n",
       " 'bodies',\n",
       " 'beyond',\n",
       " 'accumulation',\n",
       " 'knowledge',\n",
       " 'significant',\n",
       " 'takeaway',\n",
       " 'development',\n",
       " 'holistic',\n",
       " 'evaluative',\n",
       " 'skill',\n",
       " 'set',\n",
       " 'learned',\n",
       " 'assess',\n",
       " 'issues',\n",
       " 'comprehensive',\n",
       " 'biz',\n",
       " 'hss',\n",
       " 'stem',\n",
       " 'framework',\n",
       " 'found',\n",
       " 'encompassing',\n",
       " 'enough',\n",
       " 'dissect',\n",
       " 'understand',\n",
       " 'concept',\n",
       " 'industry',\n",
       " 'thoroughly',\n",
       " 'universality',\n",
       " 'framework',\n",
       " 'proved',\n",
       " 'invaluable',\n",
       " 'particularly',\n",
       " 'acquainting',\n",
       " 'unfamiliar',\n",
       " 'territories',\n",
       " 'instrumental',\n",
       " 'ventured',\n",
       " 'employer',\n",
       " 'record',\n",
       " 'eor',\n",
       " 'market',\n",
       " 'enabling',\n",
       " 'swiftly',\n",
       " 'gain',\n",
       " 'industry',\n",
       " 'insights',\n",
       " 'strategically',\n",
       " 'prepare',\n",
       " 'job',\n",
       " 'interview',\n",
       " 'potential',\n",
       " 'employer',\n",
       " 'experience',\n",
       " 'testament',\n",
       " 'practical',\n",
       " 'applicability',\n",
       " 'frameworks',\n",
       " 'concepts',\n",
       " 'explored',\n",
       " 'demonstrating',\n",
       " 'effectiveness',\n",
       " 'beyond',\n",
       " 'academics',\n",
       " 'teamwork',\n",
       " 'proposal',\n",
       " 'happened',\n",
       " 'group',\n",
       " 'hastily',\n",
       " 'prepared',\n",
       " 'first',\n",
       " 'proposal',\n",
       " 'consultation',\n",
       " 'day',\n",
       " 'last',\n",
       " 'minute',\n",
       " 'rush',\n",
       " 'stemmed',\n",
       " 'late',\n",
       " 'start',\n",
       " 'joined',\n",
       " 'group',\n",
       " 'add',\n",
       " 'drop',\n",
       " 'phase',\n",
       " 'missed',\n",
       " 'initial',\n",
       " 'discussions',\n",
       " 'role',\n",
       " 'assignments',\n",
       " 'crux',\n",
       " 'problem',\n",
       " 'poor',\n",
       " 'communication',\n",
       " 'late',\n",
       " 'addition',\n",
       " 'due',\n",
       " 'add',\n",
       " 'drop',\n",
       " 'waited',\n",
       " 'passively',\n",
       " 'direction',\n",
       " 'instead',\n",
       " 'actively',\n",
       " 'integrating',\n",
       " 'stark',\n",
       " 'contrast',\n",
       " 'proactive',\n",
       " 'stance',\n",
       " 'workplace',\n",
       " 'teams',\n",
       " 'improvements',\n",
       " 'extensions',\n",
       " 'reflection',\n",
       " 'highlights',\n",
       " 'discrepancy',\n",
       " 'approach',\n",
       " 'school',\n",
       " 'versus',\n",
       " 'work',\n",
       " 'academic',\n",
       " 'settings',\n",
       " 'rely',\n",
       " 'eventual',\n",
       " 'involvement',\n",
       " 'performance',\n",
       " 'buoyed',\n",
       " 'misplaced',\n",
       " 'sense',\n",
       " 'confidence',\n",
       " 'could',\n",
       " 'account',\n",
       " 'overconfidence',\n",
       " 'intellectual',\n",
       " 'pride',\n",
       " 'school',\n",
       " 'believe',\n",
       " 'would',\n",
       " 'involved',\n",
       " 'someway',\n",
       " 'sooner',\n",
       " 'later',\n",
       " 'scared',\n",
       " 'underperforming',\n",
       " 'internship',\n",
       " 'wise',\n",
       " 'knew',\n",
       " 'need',\n",
       " 'express',\n",
       " 'else',\n",
       " 'declined',\n",
       " 'opportunity',\n",
       " 'conclusion',\n",
       " 'core',\n",
       " 'lesson',\n",
       " 'experience',\n",
       " 'importance',\n",
       " 'taking',\n",
       " 'every',\n",
       " 'aspect',\n",
       " 'seriously',\n",
       " 'communication',\n",
       " 'gap',\n",
       " 'subsequent',\n",
       " 'teamwork',\n",
       " 'failure',\n",
       " 'attributable',\n",
       " 'attitude',\n",
       " 'issues',\n",
       " 'realization',\n",
       " 'underscores',\n",
       " 'need',\n",
       " 'engaged',\n",
       " 'responsible',\n",
       " 'approach',\n",
       " 'proposal',\n",
       " 'poster',\n",
       " 'happened',\n",
       " 'proposal',\n",
       " 'poster',\n",
       " 'met',\n",
       " 'deadlines',\n",
       " 'communication',\n",
       " 'issues',\n",
       " 'persisted',\n",
       " 'leading',\n",
       " 'disagreements',\n",
       " 'disputes',\n",
       " 'arose',\n",
       " 'work',\n",
       " 'quality',\n",
       " 'group',\n",
       " 'discussions',\n",
       " 'instance',\n",
       " 'members',\n",
       " 'dismissed',\n",
       " 'discussions',\n",
       " 'time',\n",
       " 'wasting',\n",
       " 'leading',\n",
       " 'conflicts',\n",
       " 'others',\n",
       " 'faced',\n",
       " 'rejection',\n",
       " 'contributions',\n",
       " 'due',\n",
       " 'mistrust',\n",
       " 'improvements',\n",
       " 'extensions',\n",
       " 'retrospect',\n",
       " 'clear',\n",
       " 'valuing',\n",
       " 'team',\n",
       " 'member',\n",
       " 'input',\n",
       " 'crucial',\n",
       " 'missed',\n",
       " 'opportunities',\n",
       " 'mediate',\n",
       " 'guide',\n",
       " 'team',\n",
       " 'towards',\n",
       " 'respectful',\n",
       " 'supportive',\n",
       " 'dynamic',\n",
       " 'experience',\n",
       " 'taught',\n",
       " 'importance',\n",
       " 'assuming',\n",
       " 'active',\n",
       " 'role',\n",
       " 'conflict',\n",
       " 'resolution',\n",
       " 'supporting',\n",
       " 'others',\n",
       " 'comfort',\n",
       " 'zones',\n",
       " 'conclusion',\n",
       " 'inevitably',\n",
       " 'encounter',\n",
       " 'areas',\n",
       " 'experts',\n",
       " 'vital',\n",
       " 'empathize',\n",
       " 'others',\n",
       " 'situations',\n",
       " 'offering',\n",
       " 'guidance',\n",
       " 'respect',\n",
       " 'instead',\n",
       " 'judgment',\n",
       " 'understanding',\n",
       " 'shape',\n",
       " 'future',\n",
       " 'interactions',\n",
       " 'ensuring',\n",
       " 'contribute',\n",
       " 'positively',\n",
       " 'team',\n",
       " 'cohesion',\n",
       " 'mutual',\n",
       " 'growth']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[essays[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EdeX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
